train.mat <- model.matrix(Y~., data = data.train, nvmax = 20)
## set a dummy matrix mse
mse <- rep(NA, 20)
for (i in 1:20) {
## get the coefficient for the models
coefi <- coef(regfit.full, id = i)
## get the prediction values from each of the model
pred <- train.mat[, names(coefi)] %*% coefi
mse[i] <- mean((pred - Y.train)^2)
}
plot(mse, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "b")
## Fit a 20 variable model
##Perform best subset selection on the test set,
## and plot the test set MSE associated with the best model of each size.
data.test <- data.frame(Y = Y.test, X= X.test)
#regfit.full.test <- regsubsets(Y~.,data.test, nvmax = 20)
#summary(regfit.full.test)
#names(summary(regfit.full))
test.mat <- model.matrix(Y~., data = data.test, nvmax = 20)
## set a dummy matrix mse
mse.test <- rep(NA, 20)
for (i in 1:20) {
## get the coefficient for the models
coefi.test <- coef(regfit.full, id = i)
## get the prediction values from each of the model on the test data
pred.test <- test.mat[, names(coefi.test)] %*% coefi.test
mse.test[i] <- mean((pred.test - Y.test)^2)
}
plot(mse.test, xlab = "Number of predictors", ylab = "Test MSE", pch = 19, type = "b")
### ISLR Chp 6 Question 10
## Include the lbraries
library(MASS)
library(ISLR)
library(boot)
## include leaps library for performing regression
## for best subset method
library(leaps)
### Set the seed
set.seed(1)
## genearte the predictors(20) with 1K observations
X=matrix(rnorm(1000*20),ncol=20,nrow=1000)
## create coefficients and set some of them to 0
bi <-rnorm(20)
bi[seq(2,18,by=4)] <-0
names(bi)
## genrate the variance
eps <- rnorm(1000)
## MOdel
Y <- X %*% bi + eps
### generate training (100 sample) and test 900 sample set.
train=sample(1:nrow(X),100)
test <- (-train)
X.train <- X[train, ]
X.test <- X[test, ]
Y.train <- Y[train]
Y.test <- Y[test]
## perform the best subset selection method
data.train <- data.frame(Y = Y.train, X= X.train)
regfit.full <- regsubsets(Y~.,data.train)
summary (regfit.full)
## Fit a 20 variable model
##Perform best subset selection on the training set,
## and plot the training set MSE associated with the best model of each size.
regfit.full <- regsubsets(Y~.,data.train, nvmax = 20)
summary(regfit.full)
#names(summary(regfit.full))
train.mat <- model.matrix(Y~., data = data.train, nvmax = 20)
## set a dummy matrix mse
mse <- rep(NA, 20)
for (i in 1:20) {
## get the coefficient for the models
coefi <- coef(regfit.full, id = i)
## get the prediction values from each of the model
pred <- train.mat[, names(coefi)] %*% coefi
mse[i] <- mean((pred - Y.train)^2)
}
plot(mse, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "b")
## Fit a 20 variable model
##Perform best subset selection on the test set,
## and plot the test set MSE associated with the best model of each size.
data.test <- data.frame(Y = Y.test, X= X.test)
#regfit.full.test <- regsubsets(Y~.,data.test, nvmax = 20)
#summary(regfit.full.test)
#names(summary(regfit.full))
test.mat <- model.matrix(Y~., data = data.test, nvmax = 20)
## set a dummy matrix mse
mse.test <- rep(NA, 20)
for (i in 1:20) {
## get the coefficient for the models
coefi.test <- coef(regfit.full, id = i)
## get the prediction values from each of the model on the test data
pred.test <- test.mat[, names(coefi.test)] %*% coefi.test
mse.test[i] <- mean((pred.test - Y.test)^2)
}
plot(mse.test, xlab = "Number of predictors", ylab = "Test MSE", pch = 19, type = "b")
### ISLR Chp 6 Question 10
## Include the lbraries
library(MASS)
library(ISLR)
library(boot)
## include leaps library for performing regression
## for best subset method
library(leaps)
### Set the seed
set.seed(1)
## genearte the predictors(20) with 1K observations
X=matrix(rnorm(1000*20),ncol=20,nrow=1000)
## create coefficients and set some of them to 0
bi <-rnorm(20)
bi[seq(2,18,by=4)] <-0
names(bi)
## genrate the variance
eps <- rnorm(1000)
## MOdel
Y <- X %*% bi + eps
### generate training (100 sample) and test 900 sample set.
train=sample(1:nrow(X),100)
test <- (-train)
X.train <- X[train, ]
X.test <- X[test, ]
Y.train <- Y[train]
Y.test <- Y[test]
## perform the best subset selection method
data.train <- data.frame(Y = Y.train, X= X.train)
regfit.full <- regsubsets(Y~.,data.train)
summary (regfit.full)
## Fit a 20 variable model
##Perform best subset selection on the training set,
## and plot the training set MSE associated with the best model of each size.
regfit.full <- regsubsets(Y~.,data.train, nvmax = 20)
summary(regfit.full)
#names(summary(regfit.full))
train.mat <- model.matrix(Y~., data = data.train, nvmax = 20)
## set a dummy matrix mse
mse <- rep(NA, 20)
for (i in 1:20) {
## get the coefficient for the models
coefi <- coef(regfit.full, id = i)
## get the prediction values from each of the model
pred <- train.mat[, names(coefi)] %*% coefi
mse[i] <- mean((pred - Y.train)^2)
}
plot(mse, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "b")
## Fit a 20 variable model
##Perform best subset selection on the test set,
## and plot the test set MSE associated with the best model of each size.
data.test <- data.frame(Y = Y.test, X= X.test)
#regfit.full.test <- regsubsets(Y~.,data.test, nvmax = 20)
#summary(regfit.full.test)
#names(summary(regfit.full))
test.mat <- model.matrix(Y~., data = data.test, nvmax = 20)
## set a dummy matrix mse
mse.test <- rep(NA, 20)
for (i in 1:20) {
## get the coefficient for the models
coefi.test <- coef(regfit.full, id = i)
## get the prediction values from each of the model on the test data
pred.test <- test.mat[, names(coefi.test)] %*% coefi.test
mse.test[i] <- mean((pred.test - Y.test)^2)
}
plot(mse.test, xlab = "Number of predictors", ylab = "Test MSE", pch = 19, type = "b")
### ISLR Chp 6 Question 10
## Include the lbraries
library(MASS)
library(ISLR)
library(boot)
## include leaps library for performing regression
## for best subset method
library(leaps)
### Set the seed
set.seed(1)
## genearte the predictors(20) with 1K observations
X=matrix(rnorm(1000*10),ncol=20,nrow=1000)
## create coefficients and set some of them to 0
bi <-rnorm(20)
bi[seq(2,18,by=4)] <-0
names(bi)
## genrate the variance
eps <- rnorm(1000)
## MOdel
Y <- X %*% bi + eps
### generate training (100 sample) and test 900 sample set.
train=sample(1:nrow(X),100)
test <- (-train)
X.train <- X[train, ]
X.test <- X[test, ]
Y.train <- Y[train]
Y.test <- Y[test]
## perform the best subset selection method
data.train <- data.frame(Y = Y.train, X= X.train)
regfit.full <- regsubsets(Y~.,data.train)
summary (regfit.full)
## Fit a 20 variable model
##Perform best subset selection on the training set,
## and plot the training set MSE associated with the best model of each size.
regfit.full <- regsubsets(Y~.,data.train, nvmax = 20)
summary(regfit.full)
#names(summary(regfit.full))
train.mat <- model.matrix(Y~., data = data.train, nvmax = 20)
## set a dummy matrix mse
mse <- rep(NA, 20)
for (i in 1:20) {
## get the coefficient for the models
coefi <- coef(regfit.full, id = i)
## get the prediction values from each of the model
pred <- train.mat[, names(coefi)] %*% coefi
mse[i] <- mean((pred - Y.train)^2)
}
plot(mse, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "b")
## Fit a 20 variable model
##Perform best subset selection on the test set,
## and plot the test set MSE associated with the best model of each size.
data.test <- data.frame(Y = Y.test, X= X.test)
#regfit.full.test <- regsubsets(Y~.,data.test, nvmax = 20)
#summary(regfit.full.test)
#names(summary(regfit.full))
test.mat <- model.matrix(Y~., data = data.test, nvmax = 20)
## set a dummy matrix mse
mse.test <- rep(NA, 20)
for (i in 1:20) {
## get the coefficient for the models
coefi.test <- coef(regfit.full, id = i)
## get the prediction values from each of the model on the test data
pred.test <- test.mat[, names(coefi.test)] %*% coefi.test
mse.test[i] <- mean((pred.test - Y.test)^2)
}
plot(mse.test, xlab = "Number of predictors", ylab = "Test MSE", pch = 19, type = "b")
### ISLR Chp 6 Question 10
## Include the lbraries
library(MASS)
library(ISLR)
library(boot)
## include leaps library for performing regression
## for best subset method
library(leaps)
### Set the seed
set.seed(1)
## genearte the predictors(20) with 1K observations
X=matrix(rnorm(1000*30),ncol=20,nrow=1000)
## create coefficients and set some of them to 0
bi <-rnorm(20)
bi[seq(2,18,by=4)] <-0
names(bi)
## genrate the variance
eps <- rnorm(1000)
## MOdel
Y <- X %*% bi + eps
### generate training (100 sample) and test 900 sample set.
train=sample(1:nrow(X),100)
test <- (-train)
X.train <- X[train, ]
X.test <- X[test, ]
Y.train <- Y[train]
Y.test <- Y[test]
## perform the best subset selection method
data.train <- data.frame(Y = Y.train, X= X.train)
regfit.full <- regsubsets(Y~.,data.train)
summary (regfit.full)
## Fit a 20 variable model
##Perform best subset selection on the training set,
## and plot the training set MSE associated with the best model of each size.
regfit.full <- regsubsets(Y~.,data.train, nvmax = 20)
summary(regfit.full)
#names(summary(regfit.full))
train.mat <- model.matrix(Y~., data = data.train, nvmax = 20)
## set a dummy matrix mse
mse <- rep(NA, 20)
for (i in 1:20) {
## get the coefficient for the models
coefi <- coef(regfit.full, id = i)
## get the prediction values from each of the model
pred <- train.mat[, names(coefi)] %*% coefi
mse[i] <- mean((pred - Y.train)^2)
}
plot(mse, xlab = "Number of predictors", ylab = "Training MSE", pch = 19, type = "b")
## Fit a 20 variable model
##Perform best subset selection on the test set,
## and plot the test set MSE associated with the best model of each size.
data.test <- data.frame(Y = Y.test, X= X.test)
#regfit.full.test <- regsubsets(Y~.,data.test, nvmax = 20)
#summary(regfit.full.test)
#names(summary(regfit.full))
test.mat <- model.matrix(Y~., data = data.test, nvmax = 20)
## set a dummy matrix mse
mse.test <- rep(NA, 20)
for (i in 1:20) {
## get the coefficient for the models
coefi.test <- coef(regfit.full, id = i)
## get the prediction values from each of the model on the test data
pred.test <- test.mat[, names(coefi.test)] %*% coefi.test
mse.test[i] <- mean((pred.test - Y.test)^2)
}
plot(mse.test, xlab = "Number of predictors", ylab = "Test MSE", pch = 19, type = "b")
which.min(coefi.test)
which.min(mse.test)
coef(regfit.full, which.min(val.errors))
coef(regfit.full, which.min(mse.test))
bi[seq(2,18,by=4)] <-0
seq(2,18,by=4
)
bi-coef(regfit.full, which.min(mse.test))
mse.coeff <- rep(NA, 20)
x_cols = colnames(X, do.NULL = FALSE, prefix = "X.")
for (i in 1:20) {
coefi.model <- coef(regfit.full, id = i)
mse.coeff[i] <- sqrt(sum((b[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) + sum(b[!(x_cols %in% names(coefi))])^2)
}
plot(mse.coeff, xlab = "Number of coefficients", ylab = "Error between estimated and true coefficients", pch = 19, type = "b")
mse.coeff <- rep(NA, 20)
x_cols = colnames(X, do.NULL = FALSE, prefix = "X.")
for (i in 1:20) {
coefi.model <- coef(regfit.full, id = i)
mse.coeff[i] <- sqrt(sum((b[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) + sum(b[!(x_cols %in% names(coefi))])^2)
}
mse.coeff <- rep(NA, 20)
x_cols = colnames(X, do.NULL = FALSE, prefix = "X.")
for (i in 1:20) {
coefi.model <- coef(regfit.full, id = i)
mse.coeff[i] <- sqrt(sum((bi[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) + sum(bi[!(x_cols %in% names(coefi))])^2)
}
plot(mse.coeff, xlab = "Number of coefficients", ylab = "Error between estimated and true coefficients", pch = 19, type = "b")
mse.coeff <- rep(NA, 20)
x_cols = colnames(X, do.NULL = FALSE, prefix = "X.")
for (i in 1:20) {
coefi.model <- coef(regfit.full, id = i)
mse.coeff[i] <- sqrt(sum((bi[x_cols %in% names(coefi)] - coefi.model[names(coefi) %in% x_cols])^2) + sum(bi[!(x_cols %in% names(coefi))])^2)
}
plot(mse.coeff, xlab = "Number of coefficients", ylab = "Error between estimated and true coefficients", pch = 19, type = "b")
plot(mse.coeff, xlab = "Number of coefficients", ylab = "Error between estimated and true coefficients", pch = 19, type = "b")
mse.coeff <- rep(NA, 20)
x_cols = colnames(X, do.NULL = FALSE, prefix = "X.")
for (i in 1:20) {
coefi.model <- coef(regfit.full, id = i)
mse.coeff[i] <- sqrt(sum((bi[x_cols %in% names(coefi.model)] - coefi.model[names(coefi.model) %in% x_cols])^2) + sum(bi[!(x_cols %in% names(coefi.model))])^2)
}
plot(mse.coeff, xlab = "Number of coefficients", ylab = "Error between estimated and true coefficients", pch = 19, type = "b")
library(paleocar)
library(magrittr) # The magrittr package enables piping in R.
library(raster)
library(magrittr)
library(tibble)
library(readr)
library(paleocar)
library(magrittr) # The magrittr package enables piping in R.
library(raster)
library(magrittr)
library(tibble)
library(readr)
install.packages("devtools")
devtools::install_github("bocinsky/paleocar")
library(paleocar)
update.packages("survival")
install.packages("devtools")
devtools::install_github("bocinsky/paleocar", force=TRUE)
install.packages("devtools")
library(paleocar)
library(magrittr) # The magrittr package enables piping in R.
library(raster)
library(magrittr)
library(tibble)
library(readr)
# Load spatial polygon for the boundary of Mesa Verde National Park (mvnp) in southwestern Colorado:
data(mvnp)
# Get Tree-ring data from the ITRDB for 10-degree buffer around mvnp
data(itrdb)
#fix(itrdb)
class(itrdb)
# Get 1/3 arc-second PRISM gridded data for the mvnp north study area (water-year [October--September] precipitation, in millimeters)
data(mvnp_prism)
mvnp_prism.vector <- mvnp_prism[1][1,]
test.vector <- paleocar_models(predictands = mvnp_prism.vector,
chronologies = itrdb,
calibration.years = 1920:19,
prediction.years = 1:2000,
verbose = T)
# Get 1/3 arc-second PRISM gridded data for the mvnp north study area (water-year [October--September] precipitation, in millimeters)
data(mvnp_prism)
# Extract a vector of annualized climate data (the first cell in the raster)
mvnp_prism.vector <- mvnp_prism[1][1,]
test.vector <- paleocar_models(predictands = mvnp_prism.vector,
chronologies = itrdb,
calibration.years = 1923:1984,
prediction.years = 1:2000,
verbose = T)
test.vector <- paleocar_models(predictands = mvnp_prism.vector,
chronologies = itrdb,
calibration.years = 1924:1983,
prediction.years = 1:2000,
verbose = T)
# Generate models and perform the reconstruction and error predictions.
mvnp_recon <- paleocar(predictands = mvnp_prism.vector,
label = "mvnp_prism",
chronologies = itrdb,
calibration.years = 1924:1983,
prediction.years = 1:2000,
out.dir = "./",
meanVar = "none",
floor = 0,
ceiling = NULL,
force.redo = T,
verbose = T)
#jpeg('rplot.jpg')
mvnp_recon$predictions %>%
raster::mean() %>%
raster::plot()
#dev.off ();
mvnp_recon$uncertainty %>%
raster::mean() %>%
raster::plot()
#jpeg('rplot.jpg')
mvnp_recon$predictions %>%
raster::mean() %>%
raster::plot()
#dev.off ();
# Extract a matrix of annualized climate data (all cells in the raster)
mvnp_prism.matrix <- mvnp_prism %>%
raster::as.matrix() %>%
t()
# Print to show format
mvnp_prism.matrix %>%
tibble::as_tibble()
test.matrix <- paleocar_models(predictands = mvnp_prism.matrix,
chronologies = itrdb,
calibration.years = 1924:1983,
prediction.years = 1:1985,
verbose = T)
# Generate predictions and uncertainty (and plot location means in uncertainty)
test_out_matrix<-predict_paleocar_models(models = test.matrix,
meanVar = "chained",
prediction.years = 600:1300) %>%
rowMeans() %>%
plot(x = as.numeric(names(.)),
y = .,
type = "l",
main="Predicted PPT Values - Prediction Years", xlab="Prediction Years", ylab="Prediction PPT Values"
)
# Plot the mean predictions and uncertainty
test.raster.predictions %>%
raster::mean() %>%
raster::plot(main="Predicted PPT Values - Prediction Years",
xlab="Prediction Years", ylab="Prediction PPT Values")
test.vector <- paleocar_models(predictands = mvnp_prism,
chronologies = itrdb,
calibration.years = 1924:1983,
prediction.years = 1:2000,
verbose = T)
test.raster <- paleocar_models(
predictands = mvnp_prism,
chronologies = itrdb,
calibration.years = 1924:1983,
prediction.years = 1:2000,
verbose = T
)
# Generate predictions and errors
test.raster.predictions <- predict_paleocar_models(models = test.raster,
meanVar = "chained",
prediction.years = 600:1300)
# Plot the mean predictions and uncertainty
test.raster.predictions %>%
raster::mean() %>%
raster::plot(main="Predicted PPT Values - Prediction Years",
xlab="Prediction Years", ylab="Prediction PPT Values")
edges <- c(1,2, 3,2, 2,4)
plot(graph(edges, n=max(edges), directed=TRUE))
plot(graph.extended.chordal.ring(n=100,w=10))
plot(graph.extended.chordal.ring(n=10,w=2))
library(igraph)
plot(graph.extended.chordal.ring(n=10,w=2))
>mvnp_prism
mvnp_prism
class(mvnp_prism)
install.packages('raster')
install.packages("raster")
install.packages("devtools")
devtools::install_github("bocinsky/paleocar")
library(paleocar)
library(magrittr) # The magrittr package enables piping in R.
library(raster)
library(magrittr)
library(tibble)
library(readr)
install.packages("magrittr")
library(paleocar)
library(magrittr) # The magrittr package enables piping in R.
library(raster)
library(tibble)
library(readr)
version
install.packages("raster")
install.packages("raster")
install.packages("raster")
install.packages("raster")
setwd("D:/Study/Internship/WT_PaleoCar_2017/meteor_example/wt-prov-summer-2017")
library(FedData)
library(devtools)
library(FedData)
library(devtools)
dir.create("./data-raw/MVNP", showWarnings = F, recursive = T)
# download shapefile directory
FedData::download_data("http://nrdata.nps.gov/programs/Lands/meve_tracts.zip", destdir="./data-raw/MVNP")
unlink("./data-raw/ITRDB", recursive = T)
unlink("./data-raw/MVNP", recursive=T)
library(FedData)
library(MASS)
##### GRCA Spatial Polygon
## Load spatial polygon for the boundary of Mesa Verde National Park in southwestern Colorado:
dir.create("./data-raw/GRCA", showWarnings = T, recursive = T)
FedData::download_data("http://nrdata.nps.gov/programs/Lands/GRCA_tracts.zip", destdir="./data-raw/GRCA")
# uncompress shapefile directory
utils::unzip("./data-raw/GRCA/GRCA_tracts.zip", exdir="./data-raw/GRCA/grca_tracts")
# read shapefile
grca <- rgdal::readOGR("./data-raw/GRCA/grca_tracts", layer='GRCA_boundary')
